{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb342ac-d76a-46d7-b75a-fda2a2913a6d",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence RNN Models: Translation Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5dcde1-3b93-4ba0-adae-f510c6bc4925",
   "metadata": {},
   "source": [
    "In this project, we will explore the fundamentals of sequence-to-sequence models and learn how to implement an RNN-based model for a translation task using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4d569-a02e-40dc-8dc1-dcb2aeca2cb2",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    " - Comprehend recurrent neural networks (RNN) architecture\n",
    " - Create an Encoder-Decoder model for a translation task\n",
    " - Train and evaluate the model\n",
    " - Create a generator for the translation task\n",
    " - Explain concepts related to Perplexity and BLEU score and use them for evaluating translations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4173aa0-3c99-4934-a480-7e593c2a42fb",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44ff80-dfba-48c5-800f-1751cc3e8e2b",
   "metadata": {},
   "source": [
    "### Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9ac3c-6940-47c5-bf96-ee6547901ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The installation of these packages may take several minutes. \n",
    "# Please wait patiently until the process is complete.\n",
    "# You may uncomment %%capture line below if you do not want to supress the messages that appear during package installation\n",
    "# %%capture\n",
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn\n",
    "!pip install torch==2.2.2\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install portalocker==2.8.2\n",
    "!pip install torchdata==0.7.1\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79268966-e114-403c-9f00-6e80abf71ece",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289fdab-a086-4b2b-afcb-a863b3f6384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5c43e-b780-43b7-95f2-0242163f7f55",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Sequence-to-sequence (Seq2seq) models have revolutionized various natural language processing (NLP) tasks, such as machine translation, text summarization, and chatbots. These models employ Recurrent Neural Networks (RNNs) to process variable-length input sequences and generate variable-length output sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badda2c9-d2ff-4935-b773-af3d758c0213",
   "metadata": {},
   "outputs": [],
   "source": [
    " W_xh=torch.tensor(-10.0)\n",
    " W_hh=torch.tensor(10.0)\n",
    " b_h=torch.tensor(0.0)\n",
    " x_t=1\n",
    " h_prev=torch.tensor(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437dc93-62bb-4f16-8f1a-f6b306c5c9ca",
   "metadata": {},
   "source": [
    "Considering the following sequence $x_t$ for  $t=0,1,..,7$,  then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8cb351-fe9a-4f27-9fa4-9d42302cd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[1,1,-1,-1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b9027-e38b-492b-89fa-84efa6bec647",
   "metadata": {},
   "source": [
    "Assuming that you start from the intial state $h = 0$,  with the above input vector $x$, the state vector $h$ should look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671f8ae-5368-4c5f-8f8e-62402312e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=[-1,-1,0,1,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3ce4f-1f1e-4862-8ace-1933b62ffc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the predicted state values\n",
    "H_hat = []\n",
    "# Loop through each data point in the input sequence X\n",
    "t=1\n",
    "for x in X:\n",
    "    # Assign the current data point to x_t\n",
    "    print(\"t=\",t)\n",
    "    x_t = x\n",
    "    # Print the value of the previous state (h at time t-1)\n",
    "    print(\"h_t-1\", h_prev.item())\n",
    "\n",
    "    # Compute the current state (h at time t) using the RNN formula with tanh activation\n",
    "    h_t = torch.tanh(x_t * W_xh + h_prev * W_hh + b_h)\n",
    "\n",
    "    # Update h_prev to the current state value for the next iteration\n",
    "    h_prev = h_t\n",
    "\n",
    "    # Print the current input value (x at time t)\n",
    "    print(\"x_t\", x_t)\n",
    "\n",
    "    # Print the computed state value (h at time t)\n",
    "    print(\"h_t\", h_t.item())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Append the current state value to the H_hat list after converting it to integer\n",
    "    H_hat.append(int(h_t.item()))\n",
    "    t+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca8922-5bce-4769-8d8b-c9a6d205e921",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we can evaluate the accuracy of the predicted state ```H_hat``` by comparing it to the actual state ```H```. \n",
    "In RNNs, the state $ h_t $ is utilized to predict an output sequence $y_t $ based on the given input sequence $ x_t $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca13ce-ce8d-4c34-a694-4e942ba51172",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ceac8-f6ce-4c5e-bac4-4861a20699f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d471b-59d6-43a9-abe6-281c99a7b4ea",
   "metadata": {},
   "source": [
    "While you have pre-defined the $W_{xh}$ and $W_{hh}$  and $b_h$, in practice these values need to be identified through training on data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a93481-1045-439f-93df-2b9d6bbf5378",
   "metadata": {},
   "source": [
    "In practice, modifications and enhancements, such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), are often used to address issues like the vanishing gradient problem in basic RNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46623b4-3f9e-45aa-a03b-e6ac771cc6db",
   "metadata": {},
   "source": [
    "An LSTM cell has three main components: an input gate, a forget gate, and an output gate.\n",
    "- The **input gate** controls how much new information should be stored in the cell's memory. It looks at the current input and the previous hidden state and decides which parts of the new input to remember.\n",
    "- The **forget gate** determines what information should be discarded or forgotten from the cell's memory. It considers the current input and the previous hidden state and decides which parts of the previous memory are no longer relevant.\n",
    "- The **output gate** determines what information should be outputted from the cell. It looks at the current input and the previous hidden state and decides which parts of the cell's memory to include in the output.\n",
    "\n",
    "The key idea behind LSTM cells is that they have a separate memory state that can selectively retain or forget information over time. This helps them handle long-range dependencies and remember important information from earlier steps in a sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4febaf01-89c8-4985-bdf9-3372fd241034",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence architecture\n",
    "\n",
    "Seq2seq models have an Encoder-Decoder structure. The encoder encodes the input sequence into a fixed-dimensional representation, often called the context vector($h_t$). The decoder generates the output sequence based on the encoded context vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7874e2-17e3-4855-9540-1f9705ef6e63",
   "metadata": {},
   "source": [
    "## Encoder implementation in PyTorch\n",
    "\n",
    "To implement the encoder part using Pytorch, we will create the sub-class of the torch.nn.Module class and define the __init__() and __forward__() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b8b61-54f6-43f6-b664-67d072a1f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        #input_batch = [src len, batch size]\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        embed = embed.to(device)\n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embed)\n",
    "\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afde67-6ec4-4798-8138-51f06a4fba73",
   "metadata": {},
   "source": [
    "Now we are ready to create an encoder instance to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde31245-8924-49bd-a75e-62705498b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = 8\n",
    "emb_dim = 10\n",
    "hid_dim=8\n",
    "n_layers=1\n",
    "dropout_prob=0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607908f-98e9-45d3-8833-99533af40591",
   "metadata": {},
   "source": [
    "Let's see a simple example where the encoder forward method transforms the `src` sentence into a `hidden` and `cell` states. tensor([[0],[3],[4],[2],[1]]) is equal to `src` = 0,3,4,2,1 in which each number represents a token in the `src` vocabulary. For instance, 0:`<bos>`,3:\"Das\", 4:\"ist\",2:\"schön\", 1:`<eos>`. Note that here you have batch size of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff983c37-dadc-4a00-a6cd-12f99f8f91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_batch = torch.tensor([[0,3,4,2,1]])\n",
    "# you need to transpose the input tensor as the encoder LSTM is in Sequence_first mode by default\n",
    "src_batch = src_batch.t().to(device)\n",
    "print(\"Shape of input(src) tensor:\", src_batch.shape)\n",
    "hidden_t , cell_t = encoder_t(src_batch)\n",
    "print(\"Hidden tensor from encoder:\",hidden_t ,\"\\nCell tensor from encoder:\", cell_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba1fba9-a725-430e-9867-80bbe0b5a8dd",
   "metadata": {},
   "source": [
    "## Decoder implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eef32d-2040-41c6-85b9-4ffcbac17366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "\n",
    "\n",
    "        #input = [batch size]\n",
    "\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))\n",
    "        prediction = self.softmax(prediction_logit)\n",
    "        #prediction = [batch size, output dim]\n",
    "\n",
    "\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31450977-2d85-4f1a-87f6-9972a4149bc8",
   "metadata": {},
   "source": [
    "Now we can create a decoder instance. The output dimension is set as the target vocab length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296692b-126c-424e-b74c-4985d19ec7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 6\n",
    "emb_dim=10\n",
    "hid_dim = 8\n",
    "n_layers=1\n",
    "dropout=0.5\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e9af60-e36b-439b-a97f-bd20604e5ab7",
   "metadata": {},
   "source": [
    "Now we have instances of both encoder and decoder, we are ready to connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227582bc-4004-44e2-85b7-d098aaa18e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_t = torch.tensor([0]).to(device) #<bos>\n",
    "input_t.shape\n",
    "prediction, hidden, cell = decoder_t(input_t, hidden_t , cell_t)\n",
    "print(\"Prediction:\", prediction, '\\nHidden:',hidden,'\\nCell:', cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f47f20-d944-4604-a9c0-44d4a0ac6260",
   "metadata": {},
   "source": [
    "# Encoder-decoder connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4a98b-1097-4b67-ad53-e11e82d178b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trg = [trg len, batch size]\n",
    "#teacher_forcing_ratio is probability to use teacher forcing\n",
    "#e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
    "teacher_forcing_ratio = 0.5\n",
    "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)\n",
    "\n",
    "\n",
    "batch_size = trg.shape[1]\n",
    "trg_len = trg.shape[0]\n",
    "trg_vocab_size = decoder_t.output_dim\n",
    "\n",
    "#tensor to store decoder outputs\n",
    "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "#send to device\n",
    "\n",
    "hidden_t = hidden_t.to(device)\n",
    "cell_t = cell_t.to(device)\n",
    "\n",
    "\n",
    "#first input to the decoder is the <bos> tokens\n",
    "input = trg[0,:]\n",
    "\n",
    "\n",
    "for t in range(1, trg_len):\n",
    "\n",
    "    #you loop through the trg len and generate tokens\n",
    "    #decoder receives previous generated token, cell and hidden\n",
    "    # decoder outputs it prediction(probablity distribution for the next token) and updates hidden and cell\n",
    "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
    "\n",
    "    #place predictions in a tensor holding predictions for each token\n",
    "    outputs_t[t] = output_t\n",
    "\n",
    "    #decide if you are going to use teacher forcing or not\n",
    "    teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    #get the highest predicted token from your predictions\n",
    "    top1 = output_t.argmax(1)\n",
    "\n",
    "\n",
    "    #if teacher forcing, use actual next token as next input\n",
    "    #if not, use predicted token\n",
    "    #input = trg[t] if teacher_force else top1\n",
    "    input = trg[t] if teacher_force else top1\n",
    "\n",
    "print(outputs_t,outputs_t.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd42fe-c922-4e2e-9390-b511fac2b3c4",
   "metadata": {},
   "source": [
    "The size of output tensor is (trg_len, batch_size, trg_vocab_size). This is because for each `trg` token (length of `trg`) the model outputs a probability distribution over all possible tokens(trg vocab length). Therefore, to generate the predicted tokens or translation of the `src` sentence, we need to get the maximum probability for each token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e37aa4-1094-4eb6-bac1-16028db01ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you need to get the argmax from the second dimension as **outputs** is an array of **output** tensors\n",
    "pred_tokens = outputs_t.argmax(2)\n",
    "print(pred_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6a630-6ad1-4612-b77e-8558b97f6cd9",
   "metadata": {},
   "source": [
    "It is no surprise that the translation is not correct (trg = tensor([[0],[2],[3],[5],[1]]) as the model has not yet gone through any training. Let's put together all the code for connecting the encoder and decoder in a seq2seq class for better usability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d03018-7c93-4b3e-bcb6-aff058ec8208",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence model implementation in PyTorch\n",
    "Let's connect encoder and decoder components to create the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf4089-88b7-4b3d-a5af-bf85acef4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device,trg_vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
    "\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        hidden = hidden.to(device)\n",
    "        cell = cell.to(device)\n",
    "\n",
    "\n",
    "        #first input to the decoder is the <bos> tokens\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            #decide if you are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            #get the highest predicted token from your predictions\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            #input = trg[t] if teacher_force else top1\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777f4ac-9208-42d5-8548-0e1192910c52",
   "metadata": {},
   "source": [
    "## Training model in PyTorch\n",
    "Now that the model is defined, we will define a train function for the seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307a318-a8f7-4bc0-884c-2d942a056d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Wrap iterator with tqdm for progress logging\n",
    "    train_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm progress bar with the current loss\n",
    "        train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    return epoch_loss / len(list(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f77c0c-9690-43cd-9295-f7e59ae9e1c9",
   "metadata": {},
   "source": [
    "## Evaluating model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3016854-6473-4f2f-a10f-c3230ae778dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Wrap iterator with tqdm for progress logging\n",
    "    valid_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            # Update tqdm progress bar with the current loss\n",
    "            valid_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(list(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e63cb-3e37-40a1-b2e1-4310c4816e0b",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f89bdc-b7d2-4fd5-a639-d204e1c2bd7d",
   "metadata": {},
   "source": [
    "In this section, you will fetch a language translation dataset called Multi30k, collate it (tokenization, numericalization, and adding BOS/EOS and padding) and create iterable batches of src and trg tensors.\n",
    "\n",
    "This leverages the predefined collate_fn to efficiently curate and ready batches for training the transformer model. The primary aim is to delve deeper into the intricacies of the RNN encoder and decoder components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6acc8-40cf-4e35-a4e0-6c2f3e21f9cd",
   "metadata": {},
   "source": [
    "A \"Multi30K_de_en_dataloader.py\" file has been created and I have got it while studying IBM AI Engineering Course for an assignment.  Here, we can download the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72d545-a3e6-4338-9947-83047a574764",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904b30c-deae-41b9-9b4c-3b62dbeca6a2",
   "metadata": {},
   "source": [
    "Let's run it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a43df3-6fa5-4838-9e14-cbfb23977cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Multi30K_de_en_dataloader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842134d8-eb7e-4ff6-854b-25969e7d5587",
   "metadata": {},
   "source": [
    "There we go! We only need to call the function `get_translation_dataloaders(batch_size = N,flip=True)` with an arbitrary batch size `N` and setting flip to True in order for the LSTM encoder receive input sequence in reversed order. This can help the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88ddbb-5eb7-4b2d-83bf-ca08fb500d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)#,flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf33a6-6b63-4e0e-8a2d-c764d0d69c43",
   "metadata": {},
   "source": [
    "We can check the `src` and `trg` tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbac41b-7507-402d-8baf-00ef82488f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "src, trg = next(iter(train_dataloader))\n",
    "src,trg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a5fa7-954c-4908-8594-db14bbb37fe3",
   "metadata": {},
   "source": [
    "We can also get the english and german strings using `index_to_eng` and `index_to_german` functions provided in the .py file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1b2a5-0bf4-48de-8284-2e4d70dcddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_itr = iter(train_dataloader)\n",
    "# moving forward in the dataset to reach sequences of longer length for illustration purpose. (Remember the dataset is sorted on sequence len for optimal padding)\n",
    "for n in range(1000):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "for n in range(3):\n",
    "    german, english=next(data_itr)\n",
    "    german=german.T\n",
    "    english=english.T\n",
    "    print(\"________________\")\n",
    "    print(\"german\")\n",
    "    for g in german:\n",
    "        print(index_to_german(g))\n",
    "    print(\"________________\")\n",
    "    print(\"english\")\n",
    "    for e in english:\n",
    "        print(index_to_eng(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ad256-dcb8-415f-82ce-7ab7ab96c9d5",
   "metadata": {},
   "source": [
    "* Note: When working with PyTorch tensors that represent data, it's important to understand the conventions around representing sequences. In most cases, the rows (the first dimension) in a PyTorch tensor represent individual samples, while the columns (the second dimension) represent features or time steps in the case of sequences. When dealing with sequences in PyTorch, it's common to use functions like `pad_sequence` to ensure that all sequences have the same length. Surprisingly, the padding operation is applied along the second dimension (columns), even though sequences are typically represented in the first dimension (rows). This can be confusing at first due to the way batches of sequences are represented. In many sequence-related tasks in PyTorch, especially when working with recurrent models like RNNs, LSTMs, and GRUs, batches of sequences are usually represented with the shape [sequence_length, batch_size, feature_size], where `sequence_length` refers to the length of the longest sequence within the batch(here it is equevalent to `src_len` or `trg_len`). If you check the src tensor above, you can see that the first word of of all sentences are in the first line, the second word of all sentences are in the second line, etc. That is why the first dimension is the length of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efff9b1-6db6-4284-a575-95cf33ee3b33",
   "metadata": {},
   "source": [
    "# Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de41e73-2a74-409a-96bf-d56b32b46630",
   "metadata": {},
   "source": [
    "> **Note:** **Please be aware that training the model using CPUs can be a time-consuming process. If you don't have access to GPUs, you can jump to \"Loading the saved model\" and proceed with loading the pre-trained model using the provided code. The model has been trained for five epochs and saved for your convenience and is available for download in \"Loading the saved model\" section.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf2c09-0d98-4878-87e6-1dd99ac33478",
   "metadata": {},
   "source": [
    "### Initializations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7421dd-d13f-4eef-b600-25b308294ab4",
   "metadata": {},
   "source": [
    "This code sets the random seed for various libraries and modules. This is done to make the results reproducible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f0fb3-dfdf-4884-8a86-2f9faba144e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a72a1-2fbe-470b-be0f-0238ca53ba88",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ecf44-b9bb-4bec-9d52-017142854e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform['de'])\n",
    "OUTPUT_DIM = len(vocab_transform['en'])\n",
    "ENC_EMB_DIM = 128 #256\n",
    "DEC_EMB_DIM = 128 #256\n",
    "HID_DIM = 256 #512\n",
    "N_LAYERS = 1 #2\n",
    "ENC_DROPOUT = 0.3 #0.5\n",
    "DEC_DROPOUT = 0.3 #0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86f0be-2f96-4c47-bb40-0343e7bafbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16b764-ef0e-49d4-8cc2-83d921d8511a",
   "metadata": {},
   "source": [
    "This code defines a function `count_parameters` that counts the number of trainable parameters in a given model. It then prints the count of trainable parameters in a formatted string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a485780-40e0-44c0-b03a-6a0bffc4cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06852577-7a32-441f-8475-b9f989c90805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the optimizer and loss function for training the model.\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb5d1a-7d25-4a1f-a9df-773e69d48529",
   "metadata": {},
   "source": [
    "The following helper function provides a convenient way to calculate the elapsed time in minutes and seconds given the start and end times. It will be used to measure the time taken for each epoch during training or any other time-related calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c13e34-17f1-4119-b556-78e0a6db081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57cb537-aa7f-4ecf-8cd4-26bb43738d3e",
   "metadata": {},
   "source": [
    "Let's start the training epochs:\n",
    "\n",
    "You can uncomment and execute this code in the labs supporting `CUDA` environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17748506-fb00-4ee7-a954-c3e68719dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# N_EPOCHS = 3 #run the training for at least 5 epochs\n",
    "# CLIP = 1\n",
    "\n",
    "# best_valid_loss = float('inf')\n",
    "# best_train_loss = float('inf')\n",
    "# train_losses = []\n",
    "# valid_losses = []\n",
    "\n",
    "# train_PPLs = []\n",
    "# valid_PPLs = []\n",
    "\n",
    "# for epoch in range(N_EPOCHS):\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "#     train_ppl = math.exp(train_loss)\n",
    "#     valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "#     valid_ppl = math.exp(valid_loss)\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "#     if valid_loss < best_valid_loss:\n",
    "\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'RNN-TR-model.pt')\n",
    "\n",
    "#     train_losses.append(train_loss)\n",
    "#     train_PPLs.append(train_ppl)\n",
    "#     valid_losses.append(valid_loss)\n",
    "#     valid_PPLs.append(valid_ppl)\n",
    "\n",
    "#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
    "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57b0a3-c66a-4fc6-84bc-3bb68c19e677",
   "metadata": {},
   "source": [
    "Let's visualize the model train and validation losses over the training epochs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080b2d2-c844-476f-b45e-9e4c84fb4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a list of epoch numbers\n",
    "# epochs = [epoch+1 for epoch in range(N_EPOCHS)]\n",
    "\n",
    "# # Create the figure and axes\n",
    "# fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "# ax2 = ax1.twinx()\n",
    "\n",
    "# # Plotting the training and validation loss\n",
    "# ax1.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "# ax1.plot(epochs, valid_losses, label='Validation Loss', color='orange')\n",
    "# ax1.set_xlabel('Epochs')\n",
    "# ax1.set_ylabel('Loss')\n",
    "# ax1.set_title('Training and Validation Loss/PPL')\n",
    "\n",
    "# # Plotting the training and validation perplexity\n",
    "# ax2.plot(epochs, train_PPLs, label='Train PPL', color='green')\n",
    "# ax2.plot(epochs, valid_PPLs, label='Validation PPL', color='red')\n",
    "# ax2.set_ylabel('Perplexity')\n",
    "\n",
    "# # Adjust the y-axis scaling for PPL plot\n",
    "# ax2.set_ylim(bottom=min(min(train_PPLs), min(valid_PPLs)) - 10, top=max(max(train_PPLs), max(valid_PPLs)) + 10)\n",
    "\n",
    "# # Set the legend\n",
    "# lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "# lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "# lines = lines1 + lines2\n",
    "# labels = labels1 + labels2\n",
    "# ax1.legend(lines, labels, loc='upper right')\n",
    "\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74eedb0-8453-46f8-ba05-96a09416233e",
   "metadata": {},
   "source": [
    "It can be seen that the loss and perplexity are decreasing as model gets trained. The validation loss starts to stabilize and then grow at Epoch 9, which suggests you do not need to continue training the model to avoid overtraining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2782742-96e9-4d00-afdf-93743ed8236b",
   "metadata": {},
   "source": [
    "## Loading the saved model\n",
    "If you want to skip training and load the pre-trained model instead, run the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655422a-b17c-4c63-b178-5d761724f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/RNN-TR-model.pt'\n",
    "model.load_state_dict(torch.load('RNN-TR-model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14f5fe-9089-4c9e-889b-1e7d42e7e580",
   "metadata": {},
   "source": [
    "## Model inference\n",
    "\n",
    "\n",
    "Here, we create a generator function that generates translations for input source sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93db995-0c88-435b-a44b-85eec5ff2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_translation(model, src_sentence, src_vocab, trg_vocab, max_len=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_tensor = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1).to(device)\n",
    "\n",
    "        # Pass the source tensor through the encoder\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "        # Create a tensor to store the generated translation\n",
    "        # get_stoi() maps tokens to indices\n",
    "        trg_indexes = [trg_vocab.get_stoi()['<bos>']]  # Start with <bos> token\n",
    "\n",
    "        # Convert the initial token to a PyTorch tensor\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n",
    "\n",
    "        # Move the tensor to the same device as the model\n",
    "        trg_tensor = trg_tensor.to(model.device)\n",
    "\n",
    "\n",
    "        # Generate the translation\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # Pass the target tensor and the previous hidden and cell states through the decoder\n",
    "            output, hidden, cell = model.decoder(trg_tensor[-1], hidden, cell)\n",
    "\n",
    "            # Get the predicted next token\n",
    "            pred_token = output.argmax(1)[-1].item()\n",
    "\n",
    "            # Append the predicted token to the translation\n",
    "            trg_indexes.append(pred_token)\n",
    "\n",
    "\n",
    "            # If the predicted token is the <eos> token, stop generating\n",
    "            if pred_token == trg_vocab.get_stoi()['<eos>']:\n",
    "                break\n",
    "\n",
    "            # Convert the predicted token to a PyTorch tensor\n",
    "            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n",
    "\n",
    "            # Move the tensor to the same device as the model\n",
    "            trg_tensor = trg_tensor.to(model.device)\n",
    "\n",
    "        # Convert the generated tokens to text\n",
    "        # get_itos() maps indices to tokens\n",
    "        trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]\n",
    "\n",
    "        # Remove the <sos> and <eos> from the translation\n",
    "        if trg_tokens[0] == '<bos>':\n",
    "            trg_tokens = trg_tokens[1:]\n",
    "        if trg_tokens[-1] == '<eos>':\n",
    "            trg_tokens = trg_tokens[:-1]\n",
    "\n",
    "        # Return the translation list as a string\n",
    "\n",
    "        translation = \" \".join(trg_tokens)\n",
    "\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80d2ec-62f5-4b4b-b58f-d37a5e283738",
   "metadata": {},
   "source": [
    "Now, we can check the model's output for a sample sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21a619-7b98-474a-88bb-593fe0120bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('RNN-TR-model.pt'))\n",
    "\n",
    "# Actual translation: Asian man sweeping the walkway.\n",
    "src_sentence = 'Ein asiatischer Mann kehrt den Gehweg.'\n",
    "\n",
    "\n",
    "generated_translation = generate_translation(model, src_sentence=src_sentence, src_vocab=vocab_transform['de'], trg_vocab=vocab_transform['en'], max_len=12)\n",
    "#generated_translation = \" \".join(generated_translation_list).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "print(generated_translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a591eb-117a-4dc2-b514-ee438ba40c52",
   "metadata": {},
   "source": [
    "Done! We have created a translation model that can generate german-english translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483292b1-cb02-4514-8f9c-0f524df0a278",
   "metadata": {},
   "source": [
    "## BLEU score metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db082ce-8331-426b-8930-edc77930babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(generated_translation, reference_translations):\n",
    "    # Convert the generated translations and reference translations into the expected format for sentence_bleu\n",
    "    references = [reference.split() for reference in reference_translations]\n",
    "    hypothesis = generated_translation.split()\n",
    "\n",
    "    # Calculate the BLEU score\n",
    "    bleu_score = sentence_bleu(references, hypothesis)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525dcd97-7384-4fdf-b81b-38aefd8b25f7",
   "metadata": {},
   "source": [
    "Let's calculate the BLEU score for a sample sentence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc795f-3d01-4e6e-93eb-f3e47d223275",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_translations = [\n",
    "    \"Asian man sweeping the walkway .\",\n",
    "    \"An asian man sweeping the walkway .\",\n",
    "    \"An Asian man sweeps the sidewalk .\",\n",
    "    \"An Asian man is sweeping the sidewalk .\",\n",
    "    \"An asian man is sweeping the walkway .\",\n",
    "    \"Asian man sweeping the sidewalk .\"\n",
    "]\n",
    "\n",
    "bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db8534-e64d-448d-a37e-3e4100895bf4",
   "metadata": {},
   "source": [
    "### Last check - translate a German sentence to English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ef994-b8f9-422c-83cd-538bdf86a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the German text to be translated\n",
    "german_text = \"Menschen gehen auf der Straße\"\n",
    "\n",
    "# The function should be defined to accept the text, the model, source and target vocabularies, and the device as parameters.\n",
    "english_translation = generate_translation(\n",
    "    model, \n",
    "    src_sentence=german_text, \n",
    "    src_vocab=vocab_transform['de'], \n",
    "    trg_vocab=vocab_transform['en'], \n",
    "    max_len=50\n",
    ")\n",
    "\n",
    "# Display the original and translated text\n",
    "print(f\"Original German text: {german_text}\")\n",
    "print(f\"Translated English text: {english_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9d435",
   "metadata": {},
   "source": [
    "\n",
    "## Results & Conclusion\n",
    "\n",
    "- Implemented a **Sequence-to-Sequence (Seq2Seq) model** with an **Encoder–Decoder architecture** using PyTorch.  \n",
    "- Designed for a **translation task**, showcasing how RNNs handle sequence modeling.  \n",
    "- Workflow included:\n",
    "  - Data preprocessing for sequence inputs  \n",
    "  - Building encoder and decoder RNNs  \n",
    "  - Training loop with loss evaluation  \n",
    "\n",
    "**Key Takeaways:**\n",
    "- RNN-based Seq2Seq models are effective for handling translation and sequence prediction tasks.  \n",
    "- Encoder–Decoder architecture allows flexible handling of variable-length inputs and outputs.  \n",
    "- Attention mechanisms could further enhance translation accuracy (future extension).  \n",
    "\n",
    "✅ This project demonstrates ability to:  \n",
    "- Build **custom deep learning architectures** (Encoder–Decoder)  \n",
    "- Implement **sequence modeling** in PyTorch  \n",
    "- Work with **translation-style NLP tasks**  \n",
    "\n",
    "---\n",
    "\n",
    "📌 Next Steps for Future Work:\n",
    "- Add **Attention Mechanism** to improve performance.  \n",
    "- Explore **Transformer-based Seq2Seq models** for comparison.  \n",
    "- Deploy as an **interactive demo (Streamlit or API)** to showcase live translations.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "725dd5e57e1c4473d9ac02c195dffca4320498704247d27b8d4777d54baa45d6"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
